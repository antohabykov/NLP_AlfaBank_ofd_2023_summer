{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e740225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be248c7",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa600779",
   "metadata": {},
   "source": [
    "Здесь представлена функция, чтобы размеченные данные, изначально являющиеся строками, разбивались по тегам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45c869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bio_tagging(row):\n",
    "    tokens = row[\"tokens\"]\n",
    "    good = row[\"good\"].split(',')[0].split()\n",
    "    brand = row[\"brand\"].split(',')[0].split()\n",
    "    tags = ['O'] * len(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if len(good) > 0 and tokens[i:i + len(good)] == good:\n",
    "            tags[i] = \"B-GOOD\"\n",
    "            for j in range(i + 1, i + len(good)):\n",
    "                tags[j] = \"I-GOOD\"\n",
    "        if len(brand) > 0 and tokens[i:i + len(brand)] == brand:\n",
    "            tags[i] = \"B-BRAND\"\n",
    "            for j in range(i + 1, i + len(brand)):\n",
    "                tags[j] = \"I-BRAND\"\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b873cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_tag = [\"O\", \"B-GOOD\", \"I-GOOD\", \"B-BRAND\", \"I-BRAND\", \"PAD\"]\n",
    "tag_to_index = {tag: index for index, tag in enumerate(index_to_tag)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "053a8c3d",
   "metadata": {},
   "source": [
    "# Prepare datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c5aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceiptsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, fasttext):\n",
    "        super().__init__()\n",
    "        self.is_predict = \"tags\" not in df.columns\n",
    "        self.data = df[[\"tokens\", \"good\", \"brand\", \"tags\"]] if not self.is_predict else df[[\"tokens\", \"id\"]]\n",
    "        self.data = self.data.values\n",
    "        self.fasttext = fasttext\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        identifier = 0 if not self.is_predict else self.data[index][1]\n",
    "        tokens = self.data[index][0]\n",
    "        embeddings = self.fasttext.wv[tokens]\n",
    "        goods = self.data[index][1].split(',') if not self.is_predict else list()\n",
    "        brands = self.data[index][2].split(',') if not self.is_predict else list()\n",
    "        tags = self.data[index][3] if not self.is_predict else [\"O\"] * len(tokens)\n",
    "        target = [tag_to_index[tag] for tag in tags]\n",
    "        return identifier, tokens, embeddings, goods, brands, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cad19fcc",
   "metadata": {},
   "source": [
    "Разобраться с функцией ниже, пока хз "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847269bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    ids, tokens_sequence, embeddings_sequence, goods, brands, targets = list(zip(*batch))\n",
    "    embeddings_sequence = pad_sequence([torch.FloatTensor(sequence) for sequence in embeddings_sequence], batch_first=True)\n",
    "    targets = pad_sequence([torch.LongTensor(target) for target in targets], batch_first=True, padding_value=tag_to_index[\"PAD\"])\n",
    "    return ids, tokens_sequence, embeddings_sequence, goods, brands, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75cf32e8",
   "metadata": {},
   "source": [
    "Ниже представлен типичный даталоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d6ab59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceiptsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset_path,\n",
    "                 test_dataset_path,\n",
    "                 fasttext_path,\n",
    "                 val_split_size,\n",
    "                 batch_size,\n",
    "                 num_workers):\n",
    "        super().__init__()\n",
    "        self.train_dataset_path = train_dataset_path\n",
    "        self.test_dataset_path = test_dataset_path\n",
    "        self.fasttext_path = fasttext_path\n",
    "        self.val_split_size = val_split_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.fasttext = FastText.load(self.fasttext_path)\n",
    "        self.train_df = pd.read_csv(self.train_dataset_path).fillna(\"\")\n",
    "        self.test_df = pd.read_csv(self.test_dataset_path)\n",
    "        \n",
    "        self.train_df[\"tokens\"] = self.train_df[\"name\"].str.lower().str.split()\n",
    "        self.test_df[\"tokens\"] = self.test_df[\"name\"].str.lower().str.split()\n",
    "        \n",
    "        self.train_df[\"tags\"] = self.train_df.apply(apply_bio_tagging, axis=1)\n",
    "        print(self.train_df[\"tags\"])\n",
    "    def setup(self, stage: str):\n",
    "        self.train_df, self.val_df = train_test_split(self.train_df, test_size=self.val_split_size)\n",
    "        \n",
    "        self.train_dataset = ReceiptsDataset(self.train_df, self.fasttext)\n",
    "        self.val_dataset = ReceiptsDataset(self.val_df, self.fasttext)\n",
    "        self.predict_dataset = ReceiptsDataset(self.test_df, self.fasttext)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.predict_dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "677c041d",
   "metadata": {},
   "source": [
    "Можно увеличить batch_size, если железо схавает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c3aac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"train_light_dataset.csv\"\n",
    "TEST_DATASET_PATH = \"test_light_dataset.csv\"\n",
    "FASTTEXT_PATH = \"fasttext.model\"\n",
    "VAL_SPLIT_SIZE = 0.1\n",
    "BATCH_SIZE = 11\n",
    "NUM_WORKERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c641742",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ReceiptsDataModule(\n",
    "    TRAIN_DATASET_PATH,\n",
    "    TEST_DATASET_PATH,\n",
    "    FASTTEXT_PATH,\n",
    "    VAL_SPLIT_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09717716",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db73afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "\n",
    "    def update(self, pred, target):\n",
    "        pred = frozenset(x for x in pred)\n",
    "        target = frozenset(x for x in target)\n",
    "        self.tp += len(pred & target)\n",
    "        self.fp += len(pred - target)\n",
    "        self.fn += len(target - pred)\n",
    "\n",
    "    def reset(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "\n",
    "    def get(self):\n",
    "        if self.tp == 0:\n",
    "            return 0.0\n",
    "        precision = self.tp / (self.tp + self.fp)\n",
    "        recall = self.tp / (self.tp + self.fn)\n",
    "        return 2 / (1 / precision + 1 / recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469a277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceiptsModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 rnn_input_size,\n",
    "                 rnn_hidden_size,\n",
    "                 rnn_num_layers,\n",
    "                 rnn_dropout,\n",
    "                 mlp_hidden_size,\n",
    "                 learning_rate):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lstm = nn.RNN(input_size=rnn_input_size,\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           num_layers=rnn_num_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=rnn_dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, len(index_to_tag))\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tag_to_index[\"PAD\"], reduction=\"mean\")\n",
    "        self.f1_good_train = F1Score()\n",
    "        self.f1_brand_train = F1Score()\n",
    "        self.f1_good_val = F1Score()\n",
    "        self.f1_brand_val = F1Score()\n",
    "    \n",
    "    def forward(self, sequences):\n",
    "        sequences, _ = self.lstm(sequences)\n",
    "        logits = self.mlp(sequences)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        ids, tokens_sequence, embeddings_sequence, goods, brands, targets = batch\n",
    "        logits = self(embeddings_sequence)\n",
    "        loss = self.criterion(logits.transpose(1, 2), targets)\n",
    "        tags_indices_sequence = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        for i, tags_indices in enumerate(tags_indices_sequence):\n",
    "            tags = [index_to_tag[index] for index in tags_indices[:len(tokens_sequence[i])]]\n",
    "            entities = get_entities(tags)\n",
    "            goods_pred = [' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"GOOD\"]\n",
    "            brands_pred = [' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"BRAND\"]\n",
    "            self.f1_good_train.update(goods_pred, goods[i])\n",
    "            self.f1_brand_train.update(brands_pred, brands[i])\n",
    "        self.log(\"loss/train\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"metric/f1_good_train\", self.f1_good_train.get())\n",
    "        self.log(\"metric/f1_brand_train\", self.f1_brand_train.get())\n",
    "        self.f1_good_train.reset()\n",
    "        self.f1_brand_train.reset()\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        ids, tokens_sequence, embeddings_sequence, goods, brands, targets = batch\n",
    "        logits = self(embeddings_sequence)\n",
    "        loss = self.criterion(logits.transpose(1, 2), targets)\n",
    "        tags_indices_sequence = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        for i, tags_indices in enumerate(tags_indices_sequence):\n",
    "            tags = [index_to_tag[index] for index in tags_indices[:len(tokens_sequence[i])]]\n",
    "            entities = get_entities(tags)\n",
    "            goods_pred = [' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"GOOD\"]\n",
    "            brands_pred = [' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"BRAND\"]\n",
    "            self.f1_good_val.update(goods_pred, goods[i])\n",
    "            self.f1_brand_val.update(brands_pred, brands[i])\n",
    "        self.log(\"loss/val\", loss)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"metric/f1_good_val\", self.f1_good_val.get())\n",
    "        self.log(\"metric/f1_brand_val\", self.f1_brand_val.get())\n",
    "        self.f1_good_val.reset()\n",
    "        self.f1_brand_val.reset()\n",
    "    \n",
    "    def predict_step(self, batch, _):\n",
    "        ids, tokens_sequence, embeddings_sequence, _, _, _ = batch\n",
    "        logits = self(embeddings_sequence)\n",
    "        tags_indices_sequence = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        result = list()\n",
    "        for i, tags_indices in enumerate(tags_indices_sequence):\n",
    "            tags = [index_to_tag[index] for index in tags_indices[:len(tokens_sequence[i])]]\n",
    "            entities = get_entities(tags)\n",
    "            goods_pred = ','.join([' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"GOOD\"])\n",
    "            brands_pred = ','.join([' '.join(tokens_sequence[i][start:finish + 1]) for t, start, finish in entities if t == \"BRAND\"])\n",
    "            result.append([ids[i], goods_pred, brands_pred])\n",
    "        return result\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c6e6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_INPUT_SIZE = 300\n",
    "RNN_HIDDEN_SIZE = 300\n",
    "RNN_NUM_LAYERS = 3\n",
    "RNN_DROPOUT = 0.1\n",
    "MLP_HIDDEN_SIZE = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "model = ReceiptsModule(\n",
    "    RNN_INPUT_SIZE,\n",
    "    RNN_HIDDEN_SIZE,\n",
    "    RNN_NUM_LAYERS,\n",
    "    RNN_DROPOUT,\n",
    "    MLP_HIDDEN_SIZE,\n",
    "    LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62395114",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(\"tb_logs\", name=\"ner_rnn_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87bbedf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "   accelerator='auto',\n",
    "   devices=1,\n",
    "    logger=logger,\n",
    "    # max_epochs=30,\n",
    "    max_epochs=1,\n",
    "    log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855166b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdm)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    555\u001b[0m     ckpt_path,\n\u001b[0;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m )\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:887\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m    886\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 887\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_connector\u001b[39m.\u001b[39;49mprepare_data()\n\u001b[0;32m    889\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[39m# SET UP THE TRAINER\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    892\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: setting up strategy environment\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:94\u001b[0m, in \u001b[0;36m_DataConnector.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m     dm_prepare_data_per_node \u001b[39m=\u001b[39m datamodule\u001b[39m.\u001b[39mprepare_data_per_node\n\u001b[0;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m (dm_prepare_data_per_node \u001b[39mand\u001b[39;00m local_rank_zero) \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m dm_prepare_data_per_node \u001b[39mand\u001b[39;00m global_rank_zero):\n\u001b[1;32m---> 94\u001b[0m         call\u001b[39m.\u001b[39;49m_call_lightning_datamodule_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mprepare_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     95\u001b[0m \u001b[39m# handle lightning module prepare data:\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# check for prepare_data_per_node before calling lightning_module.prepare_data\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m lightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:162\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[0;32m    161\u001b[0m     \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningDataModule]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mReceiptsDataModule.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfasttext \u001b[39m=\u001b[39m FastText\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfasttext_path)\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataset_path)\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_dataset_path)\n\u001b[0;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39mTextReader(src, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:555\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2a974",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(model, datamodule\u001b[39m=\u001b[39;49mdm)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:805\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[0;32m    803\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 805\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    806\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[0;32m    807\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:847\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[1;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[0;32m    844\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    849\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:887\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m    886\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 887\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_connector\u001b[39m.\u001b[39;49mprepare_data()\n\u001b[0;32m    889\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[39m# SET UP THE TRAINER\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    892\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: setting up strategy environment\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:94\u001b[0m, in \u001b[0;36m_DataConnector.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m     dm_prepare_data_per_node \u001b[39m=\u001b[39m datamodule\u001b[39m.\u001b[39mprepare_data_per_node\n\u001b[0;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m (dm_prepare_data_per_node \u001b[39mand\u001b[39;00m local_rank_zero) \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m dm_prepare_data_per_node \u001b[39mand\u001b[39;00m global_rank_zero):\n\u001b[1;32m---> 94\u001b[0m         call\u001b[39m.\u001b[39;49m_call_lightning_datamodule_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mprepare_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     95\u001b[0m \u001b[39m# handle lightning module prepare data:\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# check for prepare_data_per_node before calling lightning_module.prepare_data\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m lightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:162\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[0;32m    161\u001b[0m     \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningDataModule]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mReceiptsDataModule.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfasttext \u001b[39m=\u001b[39m FastText\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfasttext_path)\n\u001b[0;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset_path)\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_dataset_path)\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39mTextReader(src, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itmot\\Miniconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:555\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e3b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>good</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>торт</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>смеситель</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>лимон</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>коньяк</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4995</td>\n",
       "      <td>рамка</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4996</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4997</td>\n",
       "      <td>наконечники</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4998</td>\n",
       "      <td>шоколад</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4999</td>\n",
       "      <td>опора</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         good brand\n",
       "0        0                   \n",
       "1        1         торт      \n",
       "2        2    смеситель      \n",
       "3        3        лимон      \n",
       "4        4       коньяк      \n",
       "...    ...          ...   ...\n",
       "4995  4995        рамка      \n",
       "4996  4996                   \n",
       "4997  4997  наконечники      \n",
       "4998  4998      шоколад      \n",
       "4999  4999        опора      \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(sum(pred, list()), columns=[\"id\", \"good\", \"brand\"])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
